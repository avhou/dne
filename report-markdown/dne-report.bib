
@inproceedings{transformer,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-03-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/GIKRUUIR/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{dualstage,
	address = {Melbourne, Australia},
	title = {A {Dual}-{Stage} {Attention}-{Based} {Recurrent} {Neural} {Network} for {Time} {Series} {Prediction}},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/366},
	doi = {10.24963/ijcai.2017/366},
	abstract = {The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the ﬁrst stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction.},
	language = {en},
	urldate = {2024-03-06},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison W.},
	month = aug,
	year = {2017},
	pages = {2627--2633},
	file = {Qin et al. - 2017 - A Dual-Stage Attention-Based Recurrent Neural Netw.pdf:/Users/alexander/Zotero/storage/GY28F66P/Qin et al. - 2017 - A Dual-Stage Attention-Based Recurrent Neural Netw.pdf:application/pdf},
}

@misc{timeseries1,
	title = {Transformers in {Time} {Series}: {A} {Survey}},
	shorttitle = {Transformers in {Time} {Series}},
	url = {http://arxiv.org/abs/2202.07125},
	abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
	month = may,
	year = {2023},
	note = {arXiv:2202.07125 [cs, eess, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/alexander/Zotero/storage/7ZR93A3K/2202.html:text/html;Full Text PDF:/Users/alexander/Zotero/storage/7NFFKZRY/Wen et al. - 2023 - Transformers in Time Series A Survey.pdf:application/pdf},
}

@inproceedings{paper,
	title = {Enhancing the {Locality} and {Breaking} the {Memory} {Bottleneck} of {Transformer} on {Time} {Series} {Forecasting}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html},
	abstract = {Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dot- product self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only O(L(log L){\textasciicircum}2) memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and real- world datasets show that it compares favorably to the state-of-the-art.},
	urldate = {2024-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
	year = {2019},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/GWVJTAJ9/Li et al. - 2019 - Enhancing the Locality and Breaking the Memory Bot.pdf:application/pdf},
}

@misc{timeseries3,
	title = {Transformers predicting the future. {Applying} attention in next-frame and time series forecasting},
	url = {http://arxiv.org/abs/2108.08224},
	doi = {10.48550/arXiv.2108.08224},
	abstract = {Recurrent Neural Networks were, until recently, one of the best ways to capture the timely dependencies in sequences. However, with the introduction of the Transformer, it has been proven that an architecture with only attention-mechanisms without any RNN can improve on the results in various sequence processing tasks (e.g. NLP). Multiple studies since then have shown that similar approaches can be applied for images, point clouds, video, audio or time series forecasting. Furthermore, solutions such as the Perceiver or the Informer have been introduced to expand on the applicability of the Transformer. Our main objective is testing and evaluating the effectiveness of applying Transformer-like models on time series data, tackling susceptibility to anomalies, context awareness and space complexity by fine-tuning the hyperparameters, preprocessing the data, applying dimensionality reduction or convolutional encodings, etc. We are also looking at the problem of next-frame prediction and exploring ways to modify existing solutions in order to achieve higher performance and learn generalized knowledge.},
	urldate = {2024-03-16},
	publisher = {arXiv},
	author = {Cholakov, Radostin and Kolev, Todor},
	month = aug,
	year = {2021},
	note = {arXiv:2108.08224 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/BULRUHXZ/Cholakov and Kolev - 2021 - Transformers predicting the future. Applying atten.pdf:application/pdf;arXiv.org Snapshot:/Users/alexander/Zotero/storage/CTX7IGSQ/2108.html:text/html},
}

@article{timeseries2,
	title = {Transformer-{Based} {Deep} {Learning} {Model} for {Stock} {Price} {Prediction}: {A} {Case} {Study} on {Bangladesh} {Stock} {Market}},
	volume = {22},
	issn = {1469-0268},
	shorttitle = {Transformer-{Based} {Deep} {Learning} {Model} for {Stock} {Price} {Prediction}},
	url = {https://www.worldscientific.com/doi/full/10.1142/S146902682350013X},
	doi = {10.1142/S146902682350013X},
	abstract = {In the modern capital market, the price of a stock is often considered to be highly volatile and unpredictable because of various social, financial, political and other dynamic factors. With calculated and thoughtful investment, stock market can ensure a handsome profit with minimal capital investment, while incorrect prediction can easily bring catastrophic financial loss to the investors. This paper introduces the application of a recently introduced machine learning model — the transformer model, to predict the future price of stocks of Dhaka Stock Exchange (DSE), the leading stock exchange in Bangladesh. The transformer model has been widely leveraged for natural language processing and computer vision tasks, but, to the best of our knowledge, has never been used for stock price prediction task task using DSE data. Recently, the introduction of time2vec encoding to represent the time series features has made it possible to employ the transformer model for the stock price prediction. This paper aims to leverage these two effective techniques to discover forecasting ability on the volatile stock market of DSE. We deal with the historical daily and weekly data of eight specific stocks listed in DSE. Our experiments demonstrate promising results and acceptable root-mean-squared error on most of the stocks. We also compare the performance of our model with that of a well-known benchmark stock forecasting model called ARIMA and report satisfactory results.},
	number = {03},
	urldate = {2024-03-16},
	journal = {International Journal of Computational Intelligence and Applications},
	author = {Muhammad, Tashreef and Aftab, Anika Bintee and Ibrahim, Muhammad and Ahsan, Md. Mainul and Muhu, Maishameem Meherin and Khan, Shahidul Islam and Alam, Mohammad Shafiul},
	month = sep,
	year = {2023},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {artificial neural network, Dhaka stock exchange, Machine learning, stock price prediction, time series analysis, transformer model},
	pages = {2350013},
	file = {Submitted Version:/Users/alexander/Zotero/storage/YKTL3QQR/Muhammad et al. - 2023 - Transformer-Based Deep Learning Model for Stock Pr.pdf:application/pdf},
}

@misc{dne,
	title = {Startpagina - {IM1102}-{232433M} - {Deep} {Neural} {Engineering}},
	url = {https://brightspace.ou.nl/d2l/home/8636},
	urldate = {2024-03-21},
	file = {Startpagina - IM1102-232433M - Deep Neural Engineering:/Users/alexander/Zotero/storage/MDA4K36G/8636.html:text/html},
}

@misc{dataset,
	title = {Solar-{PV} power generation data},
	url = {https://www.elia.be/en/grid-data/power-generation/solar-pv-power-generation-data},
	language = {en},
	urldate = {2024-03-23},
	file = {Snapshot:/Users/alexander/Zotero/storage/K5SXI3T3/solar-pv-power-generation-data.html:text/html},
}

@misc{github,
	title = {avhou/dne},
	url = {https://github.com/avhou/dne},
	abstract = {DNE project 2024},
	author = {Hecke, Alexander Van and Lescrauwaet, Arne and Verschelde, Joachim},
	month = mar,
	year = {2024},
}
